#!/usr/bin/env python3
"""Quick test to verify chat uses GPU node and is fast."""

import time
from flockparsecli import load_balancer, CHAT_MODEL

print("=" * 70)
print("CHAT ROUTING TEST")
print("=" * 70)

# Show current node stats
print("\nğŸ“Š Current Node Configuration:")
for inst in load_balancer.instances:
    stats = load_balancer.instance_stats[inst]
    has_gpu = "ğŸš€ GPU" if stats.get("has_gpu") else "ğŸ¢ CPU"
    print(f"   {inst}: {has_gpu}")

# Test chat routing
print("\nğŸ¤– Testing chat generation with load balancer...")
print(f"   Model: {CHAT_MODEL}")

messages = [
    {"role": "system", "content": "You are a helpful assistant. Be brief."},
    {"role": "user", "content": "What is 2+2? Answer in one word."}
]

start = time.time()
try:
    response = load_balancer.chat_distributed(CHAT_MODEL, messages)
    duration = time.time() - start

    answer = response['message']['content']

    print(f"\nâœ… Chat Response: {answer}")
    print(f"â±ï¸  Duration: {duration:.2f}s")

    if duration < 30:
        print("âœ… PASS: Chat is fast (using GPU node)")
    else:
        print("âš ï¸  WARNING: Chat is slow (may not be using GPU)")

except Exception as e:
    print(f"âŒ Error: {e}")
    import traceback
    traceback.print_exc()

# Show which node handled it
print("\nğŸ“Š Updated Node Stats:")
for inst in load_balancer.instances:
    stats = load_balancer.instance_stats[inst]
    requests = stats.get("requests", 0)
    if requests > 0:
        print(f"   {inst}: {requests} requests")

print("\n" + "=" * 70)
